---
title: "Linear Regression"
author: "Raven McKnight"
date: "10/9/2019"
output: 
  html_document: 
    toc: true
    toc_float: true
    theme: paper
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = 'hide', warning = FALSE, message = FALSE)
packages <- c('data.table', 'rstan', 'ggplot2', 'bayesplot', 'tigris', 'sf', 'dplyr', 'shinystan')

miss_pkgs <- packages[!packages %in% installed.packages()[,1]]

if(length(miss_pkgs) > 0){
  install.packages(miss_pkgs)
}

invisible(lapply(packages, library, character.only = TRUE))

rm(miss_pkgs, packages)
rstan_options(auto_write = TRUE)

options(tigris_class = 'sf')
```

## The Data

The response variable for this stage of modeling is **average daily boardings + alightings per capita in 2017 by block group**. Later, we can incorporate time in two ways: by using averages across years *and* by using actual counts by day. The log distribution of the response is below. 
```{r}
# mod_dat <- readRDS('/Users/mcknigri/Documents/honors/honors-work/data/ag-modeling-data.RDS')
# mod_dat[, tract_GEOID := substr(GEOID, 1, 11)]
# 
# # add more recent covariates
# educ <- readRDS('/Users/mcknigri/Documents/honors/honors-work/data/covariates/education.RDS')
# house_veh <- readRDS('/Users/mcknigri/Documents/honors/honors-work/data/covariates/housing-and-vehicles.RDS')
# language <- readRDS('/Users/mcknigri/Documents/honors/honors-work/data/covariates/language.RDS')
# nativity <- readRDS('/Users/mcknigri/Documents/honors/honors-work/data/covariates/nativity.RDS')
# employment <- readRDS('/Users/mcknigri/Documents/honors/honors-work/data/covariates/wac/wac-2017.RDS')
# setDT(educ)
# setDT(house_veh)
# setDT(language)
# setDT(nativity)
# setDT(employment)
# 
# head(nativity[year==3])
# # should keep in mind that this is currently excluding all block groups with no ridership
# # should create a test set to "validate" model and then predict for places with no existing service
# mod_dat <- educ[year == 3, c('perc_hs', 'perc_bach', 'GEOID')][mod_dat, on = 'GEOID']
# mod_dat <- house_veh[year == 3, c('GEOID', 'perc_rent', 'perc_owner_occ', 'perc_no_veh')][mod_dat, on = 'GEOID']
# mod_dat <- language[year == 3, c('GEOID', 'perc_english_only')][mod_dat, on = 'GEOID']
# setnames(nativity, 'GEOID', 'tract_GEOID')
# mod_dat <- nativity[year == 3, c('tract_GEOID', 'perc_native', 'perc_foreign')][mod_dat, on = 'tract_GEOID']
# mod_dat <- employment[, c('GEOID', 'total_jobs_here')][mod_dat, on = 'GEOID']
# 
# # get land acreage to determine employment density
# options(tigris_class = 'sf')
# counties <- c('Anoka', 'Carver', 'Dakota', 'Hennepin', 'Ramsey', 'Scott', 'Washington')
# bgs <- block_groups('MN', counties, 2016)
# setDT(bgs)
# mod_dat <- bgs[, c('GEOID', 'ALAND')][mod_dat, on = 'GEOID']
# mod_dat[, sqkm := ALAND/1000000]
# mod_dat[, emp_density := total_jobs_here/sqkm]
# 
# saveRDS(mod_dat, '/Users/mcknigri/Documents/honors/honors-work/data/ag-modeling-data.RDS')
mod_dat <- readRDS('/Users/mcknigri/Documents/honors/honors-work/data/ag-modeling-data.RDS')
mod_dat <- mod_dat[!is.na(emp_density) & !is.na(perc_no_veh) & !log_avg_act == -Inf]
response <- mod_dat$log_avg_act

ggplot(mod_dat) +
  geom_histogram(aes(x=log_avg_act)) +
  theme_minimal() +
  labs(title = 'Log of average daily ridership by block group, 2017', x = 'log average activity') +
  theme(plot.title = element_text(hjust=0.5))
```

This looks *mostly* normal but is definitly left skewed by block groups with low ridership. My intuition at this point is to use gamma instead of normal but once I move into more sophisticated spatio-temporal models, I think having a normal distribution will be more important. Poisson is also an option for most spatio-temporal models which might work once I disaggregate to use daily counts rather than averages. 

Right now, I'm working with some basic demographic variables from the Census American Community Survey & LEHD data. The covariates are below and are enumerated by block group **except** for native/foreign born. Important to remember that these are *estimates*, sometimes with huge margins of error. I retained the margins of error in my data but I'm not addressing them here. 

| variable                  | meaning                                                      |
|---------------------------|--------------------------------------------------------------|
| estimate_tot_pop          | total population                                             |
| estimate_median_age       | median age of residents                                      |
| estimate_white_alone      | number of residents identified as white alone by ACS         |
| perc_white_alone          | percent of residents identified as white alone               |
| estimate_median_hh_income | median household income                                      |
| perc_hs                   | percent of residents who graduated high school or equivalent |
| perc_bach                 | percent of residents who received a bachelors degree or more |
| perc_no_veh               | percent of households who own no vehicles                    |
| perc_owner_occ            | percent of homes occupied by owners                          |
| perc_rent                 | percent of homes occupied by renters                         |
| perc_english_only         | percent of residents who report speaking only English        |
| perc_foreign              | percent of residents born outside of the US                  |
| perc_native               | percent of residents born inside the US                      |
| total_jobs_here           | number of jobs in that block group (LEHD WAC data)           |
| emp_density               | number of jobs over square km of block group                 |


## First model - Transit Market Areas

The inital inspiration for this project was Metro Transit's Market Areas, so I'll start by "refitting" that model using slightly different covariates in a Bayesian setting. The one things I don't have is a walkability metric. MetC used intersection density which is not perfect. 

This model will use *population density* ("built-in" to the new response variable), *employment density*, and *percent of households with no vehicle*. This is a basic linear regression: 

$$\begin{align} 
y &= \text{average daily boardings + alightings by block group, 2017} \\
x_1 &= \text{employment density by block group, 2017} \\
x_2 &= \text{percent of households with no vehicle by block group, 2017} \\
\\


y &\sim \text{Normal}(\alpha + \beta_1 x_1 + \beta_2 x_2, \sigma)
\end{align}$$

```{r eval = FALSE}
# Prepare data for stan
tma_dat <- list(N = length(mod_dat$log_avg_act),
                y = mod_dat$log_avg_act,
                emp = mod_dat$emp_density,
                veh = mod_dat$perc_no_veh)

# Specify model
tma_code = "
data {
 int < lower = 1 > N; // Sample size
 vector[N] emp; // Predictor
 vector[N] veh;
 vector[N] y; // Outcome
}

parameters {
 real alpha; // Intercept
 real beta1; // Slope (regression coefficients) 
 real beta2;
 real < lower = 0 > sigma; // Error SD
}

model {
 y ~ normal(alpha + emp * beta1 + veh * beta2, sigma);
}

generated quantities {
// this lets us use ppCheck in shinystan
  vector[N] y_rep ; // vector of same length as the data y
  for (n in 1:N) 
    y_rep[n] = normal_rng(alpha + emp[n]*beta1 + veh[n]*beta2, sigma) ;
}

"

# Fit model
tma_fit <- stan(model_code = tma_code, data = tma_dat, iter = 1000, verbose = FALSE)
#saveRDS(tma_fit, 'fits/tma_fit.RDS')
```

Here's the distribution of ridership vs 50 posterior draws. 
```{r}
tma_fit <- readRDS('fits/tma_fit.RDS')
response <- mod_dat$log_avg_act
launch_shinystan(tma_fit)

posterior <- extract(tma_fit)
yrep <- posterior$y_rep

pp_check(response, yrep[1:50, ], ppc_dens_overlay)
```

The assumption above that $y$ is normal is clearly not working out. 

### Same model but skew-normal

```{r eval = FALSE}
set.seed(1115)
skew_tma_code = "
data {
 int < lower = 1 > N; // Sample size
 vector[N] emp; // Predictor
 vector[N] veh;
 vector[N] y; // Outcome
}

parameters {
 real alpha; // Intercept
 real beta1; // Slope (regression coefficients) 
 real beta2;
 real < lower = 0 > omega; // Error SD
 real skew_alpha;
}

model {
 y ~ skew_normal((alpha + emp * beta1 + veh * beta2), omega, skew_alpha);
}

generated quantities {
// this lets us use ppCheck in shinystan
  vector[N] y_rep ; // vector of same length as the data y
  for (n in 1:N)
    y_rep[n] = skew_normal_rng(alpha + emp[n]*beta1 + veh[n]*beta2, omega, skew_alpha) ;
}

"

# Fit model
skew_tma_fit <- stan(model_code = skew_tma_code, data = tma_dat, iter = 1000, verbose = TRUE)
#saveRDS(skew_tma_fit, 'fits/skew_tma_fit.RDS')
```

```{r}
# try different seed
#launch_shinystan(skew_tma_fit)
skew_tma_fit <- readRDS('fits/skew_tma_fit.RDS')
skew_tma_fit

skew_posterior <- extract(skew_tma_fit)
skew_yrep <- skew_posterior$y_rep

pp_check(response, skew_yrep[1:50, ], ppc_dens_overlay)
```

Some of these are much better but something went terribly wrong with the first chain... 



