---
title: "Most Basic Models"
author: "Raven McKnight"
date: "9/30/2019"
output: 
  html_document:
    theme: paper
    code_folding: hide
---

```{r setup, warning = FALSE, message = FALSE, results = 'hide'}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
packages <- c('data.table', 'rstan', 'ggplot2', 'bayesplot', 'tigris', 'sf', 'dplyr', 'shinystan')

miss_pkgs <- packages[!packages %in% installed.packages()[,1]]

if(length(miss_pkgs) > 0){
  install.packages(miss_pkgs)
}

invisible(lapply(packages, library, character.only = TRUE))

rm(miss_pkgs, packages)
rstan_options(auto_write = TRUE)

options(tigris_class = 'sf')
counties <- c('Anoka', 'Carver', 'Dakota', 'Hennepin', 'Ramsey', 'Scott', 'Washington')
bgs <- block_groups('MN', counties, year = 2016)
bgs <- st_transform(bgs, 32615) # let's go straight to mapping
```

This document will explore some of **the most** basic models for ridership. I'll use the most aggregated data to start with, and focus just on 2017 -- the most recent year with 5-year ACS estimates. The response variable wil lbe **average daily boardings + alightings per capita by block group**. We chose per capita rather than by acre/sqaure mile because the research question Metro Transit is most interested in is *who* is riding and (maybe more importantly) who is riding *less* now than a few years ago. 

Here is a distribution of the repsonse variable: 
```{r}
# not the ideal way to read in data
apc <- readRDS('/Users/mcknigri/Documents/honors/honors-work/data/mt-data/apc-bg-ag-sum.RDS')
setDT(apc)

# restrict to 2017
apc <- apc[date_key %like% 2017]

# get averages
apc <- apc[, .(avg_activity = mean(ag_board + ag_alight)), by = GEOID]

# join to acs population
acs <- readRDS('/Users/mcknigri/Documents/honors/honors-work/data/covariates/basic_acs.RDS')
setDT(acs)
acs <- acs[year == 3]
acs
apc <- acs[apc, on = 'GEOID']

# this is our response variable for modeling at this point. 
apc <- apc[, .(avg_act_per_capita = mean(avg_activity/estimate_tot_pop)), by = 'GEOID']

ggplot(apc, aes(x=avg_act_per_capita)) +
  geom_histogram() + 
  theme_minimal() +
  labs(x = 'activity', y = 'count', title = 'Distribution of average daily activity per capita, 2017') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(apc, aes(x=log(avg_act_per_capita))) +
  geom_histogram() + 
  theme_minimal() +
  labs(x = 'activity', y = 'count', title = 'Log distribution of average daily activity per capita, 2017') +
  theme(plot.title = element_text(hjust = 0.5))
```

In general, measures of ridership should be transformed because there will always be outliers. Since we're measuring per capita, outliers might be occurring in anticipated places (downtown transit centers) *or* more rural park-and-rides. 

Here are the distributions of some of the simpler covariates we'll use. 

```{r}
ggplot(acs[variable != 'speak_only_english' & variable != 'tot_no_veh' & variable != 'tot_pop'], aes(x=estimate)) +
  geom_histogram() + 
  facet_wrap(~variable, scales = 'free') +
  theme_minimal() +
  labs(x = 'activity', y = 'count', title = 'Distribution of covariates, 2017') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(acs[variable != 'speak_only_english' & variable != 'tot_no_veh' & variable != 'tot_pop'], aes(x=log(estimate))) +
  geom_histogram() + 
  facet_wrap(~variable, scales = 'free') +
  theme_minimal() +
  labs(x = 'activity', y = 'count', title = 'Log distribution of covariates, 2017') +
  theme(plot.title = element_text(hjust = 0.5))
```

I feel like I've missed a very basic piece of information somewhere along the way: if my response is logged, should my covariates also be logged? Also, since my response is per capita, should I try to get my demographic predictors on a more similar scale? i.e., percent below poverty line rather than median income?

Let's look at percent who whose race is reported as "white alone." 
```{r}
white_alone <- acs[variable == 'white_alone']

apc[, variable := NULL]

# get population
apc <- white_alone[, c('estimate', 'GEOID')][apc, on  = 'GEOID']
setnames(apc, 'estimate', 'white_alone')

apc[, perc_white_alone := white_alone/tot_pop]

ggplot(apc, aes(x=perc_white_alone)) +
  geom_density() + 
  theme_minimal() +
  labs(x = 'activity', y = 'count', title = 'Percent "white alone", 2017') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(apc, aes(x = perc_white_alone, y = log(avg_act_per_capita))) +
  geom_point() + geom_smooth() +
  theme_minimal() +
  labs(title = 'percent white alone vs log average activity per capita') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(apc, aes(x = log(perc_white_alone), y = log(avg_act_per_capita))) +
  geom_point() + geom_smooth() +
  theme_minimal() +
  labs(title = 'log percent white alone vs log average activity per capita') +
  theme(plot.title = element_text(hjust = 0.5))
```

# Models

The idea at this point is to start with some simple linear regressions, then introduce explicit spatial methods, and then introduce time. This document is the starting point with linear regressions. 

## Ridership ~ white alone
Since this model is so simple, I'm going to write the stan code within R below. 

```{r}
# prepare the data for stan
race_dat1 <- left_join(bgs, apc, by = 'GEOID')
setDT(race_dat1)

# bottom code
race_dat1[is.na(perc_white_alone) | perc_white_alone == 0 | is.infinite(perc_white_alone), perc_white_alone := 0.01]

race_dat <- list(N = length(race_dat1$avg_act_per_capita),
                 y = log(as.numeric(race_dat1$avg_act_per_capita)),
                 x = log(as.numeric(race_dat1$perc_white_alone)))
```

```{r results = "hide"}
# write stan model 
race_mod = "
data {
 int < lower = 1 > N; // Sample size
 vector[N] x; // Predictor
 vector[N] y; // Outcome
}

parameters {
 real alpha; // Intercept
 real beta; // Slope (regression coefficients)
 real < lower = 0 > sigma; // Error SD
}

model {
 y ~ normal(alpha + x * beta , sigma);
}

"
# run!
# set verbose = TRUE for debugging
race_fit <- stan(model_code = race_mod, data = race_dat, iter = 1000, verbose = FALSE)
```
```{r}
summary(race_fit)
race_posterior <- extract(race_fit)
#str(race_posterior)

#launch_shinystan(race_fit)
```
I think I need to know more about evaluating models once I've fit them. I'm not quite sure what to make of n_eff being greater than the iterations. 

Trying unlogged predictor: 
```{r results = 'hide'}
race_dat2 <- list(N = length(race_dat1$avg_act_per_capita),
                 y = log(as.numeric(race_dat1$avg_act_per_capita)),
                 x = as.numeric(race_dat1$perc_white_alone))

racefit2 <- stan(model_code = race_mod, data = race_dat2, iter = 1000)
summary(racefit2)
```

Takes the same amount of time and produces similar estimates, n_eff is lower than iter now. 

### Median income

I think assuming a linear relationship between median income & ridership is less of a stretch. 
```{r}
med_inc <- acs[variable == 'median_hh_income']

apc[, variable := NULL]

# get population
apc <- med_inc[, c('estimate', 'GEOID')][apc, on  = 'GEOID']
setnames(apc, 'estimate', 'med_inc')


ggplot(apc, aes(x=med_inc)) +
  geom_density() + 
  theme_minimal() +
  labs(x = 'activity', y = 'count', title = 'Percent "white alone", 2017') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(apc, aes(x = med_inc, y = log(avg_act_per_capita))) +
  geom_point() + geom_smooth() +
  theme_minimal() +
  labs(title = 'median income vs log average activity per capita') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(apc, aes(x = log(med_inc), y = log(avg_act_per_capita))) +
  geom_point() + geom_smooth() +
  theme_minimal() +
  labs(title = 'log median income vs log average activity per capita') +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# prepare the data for stan
inc_dat1 <- left_join(bgs, apc, by = 'GEOID')
setDT(inc_dat1)

# bottom code
inc_dat1[is.na(med_inc) | med_inc == 0 | is.infinite(med_inc), med_inc := 0.01]

inc_dat <- list(N = length(inc_dat1$avg_act_per_capita),
                 y = log(as.numeric(inc_dat1$avg_act_per_capita)),
                 x = log(as.numeric(inc_dat1$med_inc)))
```

```{r results = 'hide'}
# write stan model 
inc_mod = "
data {
 int < lower = 1 > N; // Sample size
 vector[N] x; // Predictor
 vector[N] y; // Outcome
}

parameters {
 real alpha; // Intercept
 real beta; // Slope (regression coefficients)
 real < lower = 0 > sigma; // Error SD
}

model {
 y ~ normal(alpha + x * beta , sigma);
}

"
# run!
# set verbose = TRUE for debugging
inc_fit <- stan(model_code = inc_mod, data = inc_dat, iter = 1000, verbose = FALSE)
```

```{r}
summary(inc_fit)
```

Unlogged predictor:
```{r results = 'hide', eval = FALSE}
inc_dat2 <- list(N = length(inc_dat1$avg_act_per_capita),
                 y = log(as.numeric(inc_dat1$avg_act_per_capita)),
                 x = as.numeric(inc_dat1$med_inc))

inc_fit2 <- stan(model_code = inc_mod, data = inc_dat2, iter = 1000, verbose = FALSE)
```

So here the unlogged predictor took waaaay more time & has huge Rhat, mean_se, etc. 

## Median Age

Metro Transit ridership is really skewed towards riders under 35. Median age is a good start but we wondered about using percent of population under 35 (or wherever the census age group breaks).
```{r}
med_age <- acs[variable == 'median_age']

apc[, variable := NULL]

# get population
apc <- med_age[, c('estimate', 'GEOID')][apc, on  = 'GEOID']
setnames(apc, 'estimate', 'med_age')


ggplot(apc, aes(x=med_age)) +
  geom_density() + 
  theme_minimal() +
  labs(x = 'activity', y = 'count', title = 'Median age 2017') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(apc, aes(x = med_age, y = log(avg_act_per_capita))) +
  geom_point() + geom_smooth() +
  theme_minimal() +
  labs(title = 'median age vs log average activity per capita') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(apc, aes(x = log(med_age), y = log(avg_act_per_capita))) +
  geom_point() + geom_smooth() +
  theme_minimal() +
  labs(title = 'log median age vs log average activity per capita') +
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
# prepare the data for stan
age_dat1 <- left_join(bgs, apc, by = 'GEOID')
setDT(age_dat1)

# bottom code
age_dat1[is.na(med_age) | med_age == 0 | is.infinite(med_age), med_age := 0.01]

age_dat <- list(N = length(age_dat1$avg_act_per_capita),
                 y = log(as.numeric(age_dat1$avg_act_per_capita)),
                 x = log(as.numeric(age_dat1$med_age)))
```

```{r results = 'hide'}
# write stan model 
age_mod = "
data {
 int < lower = 1 > N; // Sample size
 vector[N] x; // Predictor
 vector[N] y; // Outcome
}

parameters {
 real alpha; // Intercept
 real beta; // Slope (regression coefficients)
 real < lower = 0 > sigma; // Error SD
}

model {
 y ~ normal(alpha + x * beta , sigma);
}

"
# run!
# set verbose = TRUE for debugging
age_fit <- stan(model_code = age_mod, data = age_dat, iter = 1000, verbose = FALSE)
```
```{r}
summary(age_fit)
```

# Multiple terms ooh la la

```{r}
age_dat1 <- na.omit(age_dat1)
inc_dat1 <- na.omit(inc_dat1)
race_dat1 <- na.omit(race_dat1)

View(apc_mod)


apc_mod <- na.omit(apc)
apc_mod[log(avg_act_per_capita) == '-Inf', avg_act_per_capita := NA]
apc_mod[log(med_age) == '-Inf', med_age := NA]
apc_mod[log(med_inc) == '-Inf', med_inc := NA]
apc_mod[log(perc_white_alone) == '-Inf', perc_white_alone := NA]
apc_mod <- na.omit(apc_mod)
# definitely a better way to do that


mod_dat <- list(N = length(apc_mod$avg_act_per_capita),
                y = log(as.numeric(apc_mod$avg_act_per_capita)),
                x1 = log(as.numeric(apc_mod$med_age)),
                x2 = log(as.numeric(apc_mod$med_inc)),
                x3 = log(as.numeric(apc_mod$perc_white_alone)))

mod_dat
```

```{r results = 'hide'}
mod_code = "
data {
 int < lower = 1 > N; // Sample size
 vector[N] x1; // Predictor
 vector[N] x2;
 vector[N] x3;
 vector[N] y; // Outcome
}

parameters {
 real alpha; // Intercept
 real beta1; // Slope (regression coefficients) 
 real beta2;
 real beta3;
 real < lower = 0 > sigma; // Error SD
}

model {
 y ~ normal(alpha + x1 * beta1 + x2 * beta2 + x3 * beta3, sigma);
}

generated quantities {
// this lets us use ppCheck in shinystan
  vector[N] y_rep ; // vector of same length as the data y
  for (n in 1:N) 
    y_rep[n] = normal_rng(alpha + x1[n]*beta1 + x2[n]*beta2 + x3[n]*beta3, sigma) ;
}

"

fit <- stan(model_code = mod_code, data = mod_dat, iter = 1000, verbose = FALSE)
```

```{r}
summary(fit)
response <- mod_dat$y
launch_shinystan(fit)

fit

#Use bayesplot
```

Ah! One big issues is that the "bottom coded" (previously na) values take over the distribution. I knew this already but I need to re-fit excluding places with no ridership. 